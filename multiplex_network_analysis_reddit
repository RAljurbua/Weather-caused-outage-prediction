import pandas as pd
import networkx as nx
import matplotlib.pyplot as plt
import seaborn as sns
import pylab
import matplotlib._pylab_helpers
from py3plex.core import multinet
import queue
import numpy as np
import warnings
warnings.filterwarnings('ignore')
from py3plex.core import multinet
from py3plex.core import random_generators
from py3plex.visualization.multilayer import *
from py3plex.visualization.colors import all_color_names,colors_default
from matplotlib.pyplot import figure
from py3plex.algorithms.community_detection import community_wrapper as cw
from collections import Counter
from sys import argv
from collections import Counter
from itertools import chain
import pandas as pd
import networkx as nx
import matplotlib.pyplot as plt
import seaborn as sns
import pylab
import matplotlib._pylab_helpers
from py3plex.core import multinet
import queue
import numpy as np
import warnings
warnings.filterwarnings('ignore')
from py3plex.visualization.multilayer import *
from py3plex.visualization.colors import all_color_names, colors_default
from matplotlib.pyplot import figure
from py3plex.algorithms.community_detection import community_wrapper as cw
from collections import Counter
from sys import argv
from itertools import chain
from py3plex.core import random_generators

##Proporties using Py3Plex
# read multiplex network
r_multiplex = multinet.multi_layer_network(network_type = "multiplex").load_network("network/twitter_multiplex_adj_list.txt", directed=False, input_type="multiplex_edges")



# basic statics
r_multiplex.basic_stats()


# get numeber of edges per type
edge_type_list = []
r_multiplex.monitor("Edge looping:")
for edge in r_multiplex.get_edges(data=True):
    edge_type_list.append(edge[3].get('type'))
print('Number of default edges= ' + str(edge_type_list.count('default')))
print('Number of mpx edges= ' + str(edge_type_list.count('mpx')))
print('Number of coupling edges= ' + str(edge_type_list.count('coupling')))


## number of uniqe nodes in each layer
node_layer_list = []
r_multiplex.monitor("Node looping:")
for node in r_multiplex.get_nodes(data = True):
    node_layer_list.append(node[0])
# for node, layer in node_layer_list:
Output = Counter(chain(*node_layer_list))
print('Number of nodes in layer 1 = ' + str(Output['1']))
print('Number of nodes in layer 2 = ' + str(Output['2']))
print('Number of nodes in layer 3 = ' + str(Output['3']))
print('Number of nodes in layer 4 = ' + str(Output['4']))
print('Number of nodes in layer 5 = ' + str(Output['5']))
print('Number of nodes in layer 6 = ' + str(Output['6']))



# get all nodes
all_nodes = list(r_multiplex.get_nodes())
all_nodes_indexed = {x:en for en,x in enumerate(all_nodes)}
all_nodes_indexed

with open('network/node_r_dict.csv', 'w') as f:
    for key in all_nodes_indexed.keys():
        f.write("%s,%s\n"%(key,all_nodes_indexed[key]))
        
        

# Total number of counties, and total number of unique nodes
county = pd.read_csv('network/county_dict.csv', sep=',', header=None)
node = pd.read_csv('network/node_r_dict.csv', sep=',', header=None)
node_uni = node[0].unique()
print('Total number of counties: ' + str(county.shape[0]))
print('Total number of unique nodes: ' + str(len(node_uni)))



# Degree centrality
centralities = r_multiplex.monoplex_nx_wrapper("degree_centrality")

# Average degree cenrality
sum = 0
for val in centralities.values():
    sum+=val

avg_centrailty = sum / len(centralities)
print('Average degree centrality: ', str(avg_centrailty))



# closeness centrality
closeness_centrality = r_multiplex.monoplex_nx_wrapper("closeness_centrality")

# Average closeness cenrality
sum = 0
for val in closeness_centrality.values():
    sum+=val

avg_closeness_centrality = sum / len(closeness_centrality)
print('Average closeness centrality: ', str(avg_closeness_centrality))



# eigenvector centrality
eigenvector_centrality = r_multiplex.monoplex_nx_wrapper("eigenvector_centrality_numpy")

# Average eigenvector centrality
sum = 0
for val in eigenvector_centrality.values():
    sum+=val

avg_eigenvector_centrality = sum / len(eigenvector_centrality)
print('Average eigenvector centrality: ', str(avg_eigenvector_centrality))


# square clustering
square_clustering = r_multiplex.monoplex_nx_wrapper("square_clustering")

# square clustering
sum = 0
for val in square_clustering.values():
    sum+=val

avg_square_clustering = sum / len(square_clustering)
print('Average square clustering ', str(avg_square_clustering))




# closeness vitality
closeness_vitality = r_multiplex.monoplex_nx_wrapper("closeness_vitality")
df1 = pd.DataFrame.from_dict(closeness_vitality, orient='index').reset_index()
df1.columns = ['node_layer', 'vitality']
df2 = pd.DataFrame(df1['node_layer'].tolist(), index=df1.index)
df2.columns = ['node', 'layer']
df1.drop(['node_layer'], axis=1)
final = pd.concat([df1, df2], axis=1)
final = final[['node', 'layer', 'vitality']]
final.to_csv('network/twitter_closeness_vitality.csv', sep=',')




# closeness vitality per layer (number of -inf)
print('Layer 1 number of -inf Closeness Vitality: ' + str(len(final[(final['layer']=='1') & (final['vitality'] == float('-inf'))])))
print('Layer 2 number of -inf Closeness Vitality: ' + str(len(final[(final['layer']=='2') & (final['vitality'] == float('-inf'))])))
print('Layer 3 number of -inf Closeness Vitality: ' + str(len(final[(final['layer']=='3') & (final['vitality'] == float('-inf'))])))
print('Layer 4 number of -inf Closeness Vitality: ' + str(len(final[(final['layer']=='4') & (final['vitality'] == float('-inf'))])))
print('Layer 5 number of -inf Closeness Vitality: ' + str(len(final[(final['layer']=='5') & (final['vitality'] == float('-inf'))])))
print('Layer 6 number of -inf Closeness Vitality: ' + str(len(final[(final['layer']=='6') & (final['vitality'] == float('-inf'))])))



# Aggregating information across layers. 
# Let that be the information bound to nodes or edges, 
# both can be aggregated into a single homogeneous network that can readily be analysed.

# simple networkx object
aggregated_network1 = r_multiplex.aggregate_edges(metric="count",normalize_by="degree")
print(nx.info(aggregated_network1))

# unnormalized counts for edge weights
aggregated_network2 = r_multiplex.aggregate_edges(metric="count",normalize_by="raw")
print(nx.info(aggregated_network2))

# The two networks have the same number of links (all)
# However, the weights differ!

print('Aggregated network unnormalized weight')
for e in aggregated_network2.edges(data=True):
    print(e)

print('Aggregated network normalized weight')
for e in aggregated_network1.edges(data=True):
    print(e)
    
    

# Network traversal
# The locations of the random walkers traversing across intra- as well as inter-layer edges are considered


## seed node
all_nodes = list(r_multiplex.get_nodes())
all_nodes_indexed = {x:en for en,x in enumerate(all_nodes)}
all_edges = list(r_multiplex.get_nodes())
## spread from a random node
random_init = np.random.randint(len(all_nodes))
random_node = all_nodes[random_init]
spread_vector = np.zeros(len(r_multiplex.core_network))
Q = queue.Queue(maxsize=len(all_edges))
Q.put(random_node)

layer_visit_sequence = []
node_visit_sequence = []
iterations = 0

while True:
    if not Q.empty():
        candidate = Q.get()
        iterations+=1
        if iterations % 100 == 0:
            print("Iterations: {}".format(iterations))
        for neighbor in r_multiplex.get_neighbors(candidate[0],candidate[1]):
            idx = all_nodes_indexed[neighbor]
            if spread_vector[idx] != 1:
                layer_visit_sequence.append(candidate[1])
                node_visit_sequence.append((neighbor,iterations))
                Q.put(neighbor)
                spread_vector[idx] = 1

    else:
        break

sns.distplot(layer_visit_sequence)
plt.xlabel("Layer")
plt.ylabel("Visit density")
plt.xticks([1, 2, 3, 4, 5, 6])
plt.show()



## Analyzing individual layers using networkx
df = pd.read_csv('network/reddit_multiplex_adj_list.txt', sep=' ', header=None)
# df.drop(0, inplace=True, axis=1)

df_l1 = df[df[0] == 1]
df_l2 = df[df[0] == 2]
df_l3 = df[df[0] == 3]
df_l4 = df[df[0] == 4]
df_l5 = df[df[0] == 5]
df_l6 = df[df[0] == 6]


l1 = nx.Graph()
l1 = nx.from_pandas_edgelist(df_l1, 1, 2, [3])

l2 = nx.Graph()
l2 = nx.from_pandas_edgelist(df_l2,1, 2, [3])

l3 = nx.Graph()
l3 = nx.from_pandas_edgelist(df_l3, 1, 2, [3])

l4 = nx.Graph()
l4 = nx.from_pandas_edgelist(df_l4, 1, 2, [3])

l5 = nx.Graph()
l5 = nx.from_pandas_edgelist(df_l5, 1, 2, [3])

l6 = nx.Graph()
l6 = nx.from_pandas_edgelist(df_l6,1, 2, [3])

out_file = open('network/reddit_each_layer_network_properties.txt', 'w')
print('Reddit Network Properties ', file=out_file)
print('---------------------------------------', file=out_file)

layer = [l1, l2, l3,l4,l5,l6]
layer_name = ['l1', 'l2', 'l3','l4', 'l5', 'l6']
for l, l_name in zip(layer, layer_name):
    print('Properties for layer ' + l_name , file=out_file)
    print('---------------------------------------', file=out_file)

    # General network properties
    print(' # Nodes in layer ' + l_name + ': '+ str(l.number_of_nodes()), file=out_file)
    print(' # Edges in layer ' + l_name + ': '+ str(l.number_of_edges()), file=out_file)

    # Node Degree
    degree = l.degree()
    sum_degree = 0
    for i, j in degree:
        sum_degree+=j
    print(' Avg Node degree in layer ' + l_name + ': '+ str(sum_degree/l.number_of_nodes()), file=out_file)

    # Density
    print(' Density in layer ' + l_name + ': '+ str(nx.density(l)), file=out_file)

    # Diameter
    Gcc = sorted(nx.connected_components(l), key=len, reverse=True)
    largest_cc = l.subgraph(Gcc[0])
    print(' Diameter for the largest component in layer ' + l_name + ': '+ str(nx.diameter(largest_cc)), file=out_file)

    # Clustering
    # print(' Triangles: ' + str(nx.triangles(G)) + '\n', file=out_file)
    print(' Transitivity in layer ' + l_name + ': '+ str(nx.transitivity(l)), file=out_file)
    # print(' Clustering: ' + str(nx.clustering(G)) + '\n', file=out_file)
    print(' Average Clustering in layer ' + l_name + ': '+ str(nx.average_clustering(l)), file=out_file)
    # print(' Square Clustering: ' + str(nx.square_clustering(G)) + '\n', file=out_file)
    # print(' Generalized Degree: ' + str(nx.generalized_degree(G)) + '\n', file=out_file)
    print(' Assortativity Coefficient in layer ' + l_name + ': '+ str(nx.degree_assortativity_coefficient(l)), file=out_file)


    # Degree
    deg_cent = nx.degree_centrality(l)
    sum_deg_cent = 0
    for key in deg_cent:
        sum_deg_cent+=deg_cent[key]
    print(' Avg Degree Centrality in layer ' + l_name + ': '+ str(sum_deg_cent/l.number_of_nodes()), file=out_file)
    print('\n', file=out_file)
    # print(' Degree Centrality in layer ' + l_name + ': '+ str(deg_cent), file=out_file)


    # # Eigenvector
    # # # print(' Katz Centrality: ' + str(nx.katz_centrality(G)) + '\n', file=out_file)
    # eig_cent = nx.eigenvector_centrality(G)
    # sum_eigenvector = 0
    # for key in eig_cent:
    #     sum_eigenvector+=eig_cent[key]
    # print(' Avg Eigenvector Centrality: ' + str(sum_eigenvector/G.number_of_nodes()) + '\n', file=out_file)
    # print(' Eigenvector Centrality: ' + str(eig_cent) + '\n', file=out_file)
    # print('\n', file=out_file)



    # # Closeness
    # print(' Closeness Centrality: ' + str(nx.closeness_centrality(G)), file=out_file)
    # print('\n', file=out_file)

    # # Betweenness
    # # print(' Betweenness Centrality: ' + str(nx.betweenness_centrality(G)) + '\n', file=out_file)
    # # print(' Edge Betweenness Centrality: ' + str(nx.edge_betweenness_centrality(G)) + '\n', file=out_file)
    # # print('\n\n', file=out_file)

    # # PageRank
    # print(' Page Rank: ' + str(nx.pagerank(G)) + '\n', file=out_file)
    # print('\n', file=out_file)

    # # closeness vitality
    # print(' closeness vitality: ' + str(nx.closeness_vitality(G)) + '\n', file=out_file)
    # print('\n', file=out_file)

    # # VoteRank
    # print(' Vote Rank: ' + str(nx.voterank(G)) + '\n', file=out_file)
    # print('\n', file=out_file)

out_file.close()
