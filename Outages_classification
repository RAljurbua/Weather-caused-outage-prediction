import glob
import pandas as pd
import random
from random import shuffle
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score
from sklearn.linear_model import LogisticRegression
from sklearn import tree
from sklearn.preprocessing import MinMaxScaler
import numpy as np
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as imbpipeline
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import GridSearchCV, StratifiedKFold
import matplotlib.pyplot as plt
import os
import chardet
from tqdm import tqdm
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import matplotlib.pyplot as plt
tqdm.pandas(desc="progress-bar")
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import GridSearchCV, StratifiedKFold
from stellargraph.data import BiasedRandomWalk
from stellargraph import StellarGraph
from gensim.models import Word2Vec
from imblearn.over_sampling import RandomOverSampler
from sklearn.inspection import permutation_importance
import warnings
warnings.filterwarnings('ignore')

# dataset information
def info (df, out_file):
    print('=============== Number of examples ===============', file=out_file)
    print(' Dataset Info:', file=out_file)
    print('  Number of label 0 examples = ' + str(df[df['label'] == 0].shape[0]), file=out_file)
    print('  Percentage of label 0 examples = ' + str((df[df['label'] == 0].shape[0]/df.shape[0])*100), file=out_file)
    print('  Number of label 1 examples = ' + str(df[df['label'] == 1].shape[0]), file=out_file)
    print('  Percentage of label 1 examples = ' + str(((df[df['label'] == 1].shape[0]/df.shape[0])*100)), file=out_file)
# Logistic Regression model with SMOTE for imbalance dataset
def training_logistic_SMOTE(df, out_file, i):
    
    # shuffle data
    df = df.sample(frac=1)
   
    X = df[['', '' ]].copy() #features
   
    y = df['label'].copy()

    # split training and testing and start training and prediction before oversampling
    X_train, X_test, y_train, y_test = train_test_split(
        X,
        y,
        test_size=0.25,
        stratify=y,
        random_state=11
    )

    # Deal with imbalance using SMOTE
    smote = SMOTE(random_state=11)
    X_train, y_train = smote.fit_resample(X_train, y_train)

    pipeline = Pipeline(steps=[
        ['scaler', MinMaxScaler()],
        ['classifier', LogisticRegression(random_state=11,max_iter=1000)]
    ])

    stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=11)

    param_grid = {'classifier__C': [100]}
    grid_search = GridSearchCV(
        estimator=pipeline,
        param_grid=param_grid,
        scoring='roc_auc',
        cv=stratified_kfold,
        n_jobs=-1
    )

    grid_search.fit(X_train, y_train)
    y_pred = grid_search.predict(X_test)
    cv_score = grid_search.best_score_
    test_score = grid_search.score(X_test, y_test)
    

       
    # Feature importance
    coefs = grid_search.best_estimator_.named_steps['classifier'].coef_
    feature_importance = {X.columns[i]: coef for i, coef in enumerate(coefs[0])}
    
    
    display(feature_importance)



    # save results to file
    print('\n LR Testing Results ' + str(i) + ': ', file=out_file)
    print('  Testing accuracy %s' % accuracy_score(y_test, y_pred), file=out_file)
    print('  Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')), file=out_file)
    print(file=out_file)

def main ():
    pd.set_option('display.max_column', None)
    pd.set_option('display.max_rows', None)
    pd.set_option('display.max_colwidth', None)
    

    #read data
    files = os.path.join('.///*.csv')
    files = glob.glob(files)
    df = pd.concat(map(pd.read_csv, files), ignore_index=True)
    
    
    # replace NaN values with 0
    df = df.fillna(0)
    
    out_file = open('.', 'w')
    info (df, out_file)
    
    
    # LR with SMOTE
    print(file=out_file)
    print('=============== LR WITH SMOTE ===============', file=out_file)
    for i in range(10):
        training_logistic_SMOTE(df, out_file, i)
    print('Done training logistic regression with SMOTE...')
        


if __name__ == '__main__':
    main()
 
